{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import bilby\n",
    "import nmma\n",
    "\n",
    "import astropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from astropy.time import Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: AstropyDeprecationWarning: The update_default_config function is deprecated and may be removed in a future version. [sncosmo]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install afterglowpy if you want to timeout simulations.\n"
     ]
    }
   ],
   "source": [
    "from nmma.em.model import SimpleKilonovaLightCurveModel,GRBLightCurveModel, SVDLightCurveModel, KilonovaGRBLightCurveModel, GenericCombineLightCurveModel, SupernovaLightCurveModel, ShockCoolingLightCurveModel\n",
    "\n",
    "from nmma.em.injection import create_light_curve_data as cld\n",
    "\n",
    "snModel = lambda t: SupernovaLightCurveModel(sample_times=t, model='nugent-hyper')\n",
    "grbModel = lambda t: GRBLightCurveModel(sample_times=t, model='TrPi2018')\n",
    "knModel = lambda t: SVDLightCurveModel(sample_times=t, model='Bu2019lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/anaconda3/envs/nmma/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator GaussianProcessRegressor from version 0.23.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SupernovaLightCurveModel(model={self.model})\n"
     ]
    }
   ],
   "source": [
    "modelDict = lambda t: {'nugent-hyper':snModel(t), 'TrPi2018':grbModel(t), 'Bu2019lm':knModel(t)}\n",
    "print(modelDict([1])['nugent-hyper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelDict([1])['Bu2019lm'].generate_lightcurve(sample_times=np.array([1]), parameters=best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def luminosity(distance, mag):\n",
    "    \"\"\"\n",
    "    Calculate the luminosity of a source given its distance and apparent magnitude.\n",
    "    Parameters\n",
    "    \"\"\"\n",
    "    abs = lambda mag, distance: mag - 5 * np.log10(distance * 1e6 / 10.0)\n",
    "    if type(mag) == np.ndarray:\n",
    "        return abs(mag, distance)\n",
    "    elif type(mag) == dict:\n",
    "        return {k: abs(mag[k], distance) for k in mag.keys()} \n",
    "\n",
    "def read_json(path_to_file):\n",
    "    with open(path_to_file) as p:\n",
    "        return json.load(p, object_hook=bilby.core.utils.decode_bilby_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 189 results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = glob.glob('./fits/*/*result.json')\n",
    "print(\"Found {} results\".format(len(results)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 2.32 GB\n",
      "JSON size: 0.75 GB\n",
      "json files occupy 32.42% of the total size\n"
     ]
    }
   ],
   "source": [
    "root_directory = Path('./fits/')\n",
    "total_size = sum(f.stat().st_size for f in root_directory.glob('**/*') if f.is_file()) / 1024**3\n",
    "print(\"Total size: {:.2f} GB\".format(total_size))\n",
    "json_size = sum(f.stat().st_size for f in root_directory.glob('**/*result.json') if f.is_file()) / 1024**3\n",
    "print(\"JSON size: {:.2f} GB\".format(json_size))\n",
    "print('json files occupy {:.2f}% of the total size'.format(json_size / total_size * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 189 json files took 10.47 seconds \n",
      "(average 0.06 sec/file)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "jsons = [read_json(f) for f in results]\n",
    "stop = time.time()\n",
    "print(\"Reading {} json files took {:.2f} seconds \\n(average {:.2f} sec/file)\".format(len(jsons), stop - start, (stop - start) / len(jsons)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bu2019lm'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsons[0]['label'].split('_')[0] + '_' + jsons[0]['label'].split('_')[1] + '_' + jsons[0]['label'].split('_')[-2] + '_' + jsons[0]['label'].split('_')[-1]\n",
    "jsons[0]['label'].split('_')[3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = [j['posterior'] for j in jsons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best log likelihood evaluation: 7906\n",
      "-21.183629314906504\n",
      "[ 9.48264161 49.1045723   0.77937232 -1.93927027 -2.50605828 -1.32867461\n",
      "  0.16131121]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'luminosity_distance': 9.482641609762611,\n",
       " 'KNphi': 49.10457230173926,\n",
       " 'inclination_EM': 0.7793723226107803,\n",
       " 'KNtimeshift': -1.9392702697004678,\n",
       " 'log10_mej_dyn': -2.5060582838342813,\n",
       " 'log10_mej_wind': -1.3286746136344751,\n",
       " 'Ebv': 0.16131120572259705}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ll_idx = np.argmin(np.abs(jsons[0]['log_likelihood_evaluations']))\n",
    "print(\"Best log likelihood evaluation: {}\".format(ll_idx))\n",
    "print(jsons[0]['log_likelihood_evaluations'][ll_idx])\n",
    "print(jsons[0]['samples'][ll_idx])\n",
    "best_params = dict(zip(jsons[0]['search_parameter_keys'], jsons[0]['samples'][ll_idx]))\n",
    "display(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lc(file, tmax=False):\n",
    "    '''imports dat file as a pandas dataframe'''\n",
    "    df = pd.read_csv(file, sep=' ', header=None, names=['t', 'filter', 'mag', 'mag_unc'])\n",
    "    df = df[df['mag_unc'] != np.inf] ## drop non-detections\n",
    "    df['t'] = Time(pd.to_datetime(df['t'])).mjd ## convert to mjd\n",
    "    df['t'] = df['t'] - df['t'].min() ## set t=0 to first observation\n",
    "    if tmax:\n",
    "        df = df[df['t'] < tmax]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>filter</th>\n",
       "      <th>mag</th>\n",
       "      <th>mag_unc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>g</td>\n",
       "      <td>12.364482</td>\n",
       "      <td>0.022590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.960209</td>\n",
       "      <td>g</td>\n",
       "      <td>12.654026</td>\n",
       "      <td>0.058831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.946964</td>\n",
       "      <td>g</td>\n",
       "      <td>14.291193</td>\n",
       "      <td>0.039201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.926730</td>\n",
       "      <td>g</td>\n",
       "      <td>15.711854</td>\n",
       "      <td>0.025181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.921960</td>\n",
       "      <td>g</td>\n",
       "      <td>17.933233</td>\n",
       "      <td>0.016180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11.921213</td>\n",
       "      <td>g</td>\n",
       "      <td>20.109354</td>\n",
       "      <td>0.147486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           t filter        mag   mag_unc\n",
       "0   0.000000      g  12.364482  0.022590\n",
       "1   1.960209      g  12.654026  0.058831\n",
       "2   3.946964      g  14.291193  0.039201\n",
       "3   5.926730      g  15.711854  0.025181\n",
       "4   8.921960      g  17.933233  0.016180\n",
       "5  11.921213      g  20.109354  0.147486"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lc('./injection_sample/lc_Bu2019lm_0.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params(json, verbose=False):\n",
    "    '''Get the best fit parameters from a bilby json file, return as a dictionary'''\n",
    "    ll_idx = np.argmin(np.abs(json['log_likelihood_evaluations']))\n",
    "    best_ll = json['log_likelihood_evaluations'][ll_idx]\n",
    "    print(\"Best log likelihood evaluation: {}\".format(best_ll)) if verbose else None\n",
    "    log_evidence = json['log_evidence']\n",
    "    log_evidence_err = json['log_evidence_err']\n",
    "    log_bayes_factor = json['log_bayes_factor']\n",
    "    likelihood_dict = dict(zip(['log_likelihood','log_evidence', 'log_evidence_err', 'log_bayes_factor'], [best_ll, log_evidence, log_evidence_err, log_bayes_factor]))\n",
    "    # print(bp_dict) if verbose else None\n",
    "    #bp_dict = dict(zip(json['search_parameter_keys'], json['samples'][ll_idx]))\n",
    "    bp_dict = dict(zip(json['search_parameter_keys'], json['samples'][ll_idx, :]))\n",
    "\n",
    "    return bp_dict, likelihood_dict\n",
    "\n",
    "def get_labels(json):\n",
    "    '''get the object label from bilby file, return as a dictionary'''\n",
    "    raw_label = json['label']\n",
    "    candidate = raw_label.split('_')[0] + '_' + raw_label.split('_')[1]\n",
    "    model = raw_label.split('_')[3]\n",
    "    tmax = float(raw_label.split('_')[5])\n",
    "    keys = ['candidate', 'model', 'tmax']\n",
    "    label_dict = dict(zip(keys, [candidate, model, tmax]))\n",
    "    \n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_lc(json, model, sample_times, verbose=False):\n",
    "    '''Generate a light curve from a bilby json file'''\n",
    "    bp, like_dict = get_best_params(json, verbose=verbose)\n",
    "    model_type = get_labels(json)['model']\n",
    "    model = modelDict(sample_times)[model_type]\n",
    "    print(model) if verbose else None\n",
    "    label = get_labels(json)\n",
    "    print(label) if verbose else None\n",
    "    print(bp) if verbose else None\n",
    "    lc = model.generate_lightcurve(sample_times, parameters=bp)[1]\n",
    "    lc_abs = luminosity(bp['luminosity_distance'], lc)\n",
    "    return lc_abs, label\n",
    "\n",
    "def calc_resids(lc, data):\n",
    "    '''Calculate residuals between a light curve and data'''\n",
    "    # assert len(lc) == len(data):\n",
    "    resids = 0\n",
    "    for filter in data['filter'].unique():\n",
    "        t_sample = data[data['filter'] == filter]['t']\n",
    "        lc_filt = lc[filter] #gen_lc(json, modelDict(t_sample)['Bu2019lm'], t_sample)[1]\n",
    "        data_filt = data[data['filter'] == filter]\n",
    "        resids += np.sum(np.abs(lc_filt - data_filt['mag']))/ data_filt['mag_unc']/len(lc_filt)\n",
    "    return resids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_series(json, residuals=False):\n",
    "    '''creates a pandas series from a bilby json file (already read in)'''\n",
    "    label_dict = get_labels(json)\n",
    "    bp_dict, likelihood_dict = get_best_params(json)\n",
    "    obj_dict = {**label_dict,  **likelihood_dict, **bp_dict,}\n",
    "    obj_series = pd.Series(obj_dict)\n",
    "    \n",
    "    if residuals:\n",
    "        # print(bp_dict)\n",
    "        tmax = label_dict['tmax']\n",
    "        data = get_lc('./injection_sample/lc_{}.dat'.format(label_dict['candidate']),\n",
    "                      tmax=tmax) ## should be a function argument\n",
    "        t_sample = np.array(data['t'])\n",
    "        # print(type(t_sample))\n",
    "        model = modelDict(t_sample)[label_dict['model']]\n",
    "        # print(model)\n",
    "        bf_lc, _ = gen_lc(json, model, t_sample)\n",
    "        resids = calc_resids(bf_lc, data)\n",
    "        obj_series['residuals'] = resids\n",
    "    return obj_series\n",
    "\n",
    "def create_df(jsons, residuals=False):\n",
    "    '''creates a pandas dataframe from a list of bilby json files (already read in)'''\n",
    "    return pd.DataFrame([create_series(j, residuals) for j in jsons]).sort_values(by=['candidate','model','tmax']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataframe (no residuals) took 0.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/anaconda3/envs/nmma/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator GaussianProcessRegressor from version 0.23.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/tyler/anaconda3/envs/nmma/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator GaussianProcessRegressor from version 0.23.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/tyler/anaconda3/envs/nmma/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator GaussianProcessRegressor from version 0.23.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mfit_results.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 8\u001b[0m df \u001b[39m=\u001b[39m create_df(jsons, residuals\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      9\u001b[0m t1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCreating dataframe (with residuals) took \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(t1\u001b[39m-\u001b[39mt0))\n",
      "Cell \u001b[0;32mIn[20], line 24\u001b[0m, in \u001b[0;36mcreate_df\u001b[0;34m(jsons, residuals)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_df\u001b[39m(jsons, residuals\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     23\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''creates a pandas dataframe from a list of bilby json files (already read in)'''\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame([create_series(j, residuals) \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m jsons])\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mcandidate\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtmax\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[20], line 24\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_df\u001b[39m(jsons, residuals\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     23\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''creates a pandas dataframe from a list of bilby json files (already read in)'''\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame([create_series(j, residuals) \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m jsons])\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mcandidate\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtmax\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m, in \u001b[0;36mcreate_series\u001b[0;34m(json, residuals)\u001b[0m\n\u001b[1;32m     15\u001b[0m model \u001b[39m=\u001b[39m modelDict(t_sample)[label_dict[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     16\u001b[0m \u001b[39m# print(model)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m bf_lc, _ \u001b[39m=\u001b[39m gen_lc(json, model, t_sample)\n\u001b[1;32m     18\u001b[0m resids \u001b[39m=\u001b[39m calc_resids(bf_lc, data)\n\u001b[1;32m     19\u001b[0m obj_series[\u001b[39m'\u001b[39m\u001b[39mresiduals\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m resids\n",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m, in \u001b[0;36mgen_lc\u001b[0;34m(json, model, sample_times, verbose)\u001b[0m\n\u001b[1;32m      3\u001b[0m bp, like_dict \u001b[39m=\u001b[39m get_best_params(json, verbose\u001b[39m=\u001b[39mverbose)\n\u001b[1;32m      4\u001b[0m model_type \u001b[39m=\u001b[39m get_labels(json)[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m modelDict(sample_times)[model_type]\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(model) \u001b[39mif\u001b[39;00m verbose \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      7\u001b[0m label \u001b[39m=\u001b[39m get_labels(json)\n",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m modelDict \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m t: {\u001b[39m'\u001b[39m\u001b[39mnugent-hyper\u001b[39m\u001b[39m'\u001b[39m:snModel(t), \u001b[39m'\u001b[39m\u001b[39mTrPi2018\u001b[39m\u001b[39m'\u001b[39m:grbModel(t), \u001b[39m'\u001b[39m\u001b[39mBu2019lm\u001b[39m\u001b[39m'\u001b[39m:knModel(t)}\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(modelDict([\u001b[39m1\u001b[39m])[\u001b[39m'\u001b[39m\u001b[39mnugent-hyper\u001b[39m\u001b[39m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      5\u001b[0m snModel \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m t: SupernovaLightCurveModel(sample_times\u001b[39m=\u001b[39mt, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnugent-hyper\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m grbModel \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m t: GRBLightCurveModel(sample_times\u001b[39m=\u001b[39mt, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrPi2018\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m knModel \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m t: SVDLightCurveModel(sample_times\u001b[39m=\u001b[39;49mt, model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mBu2019lm\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nmma/lib/python3.8/site-packages/nmma/em/model.py:168\u001b[0m, in \u001b[0;36mSVDLightCurveModel.__init__\u001b[0;34m(self, model, sample_times, svd_path, parameter_conversion, mag_ncoeff, lbol_ncoeff, interpolation_type, model_parameters)\u001b[0m\n\u001b[1;32m    166\u001b[0m mag_modelfile \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msvd_path, \u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m_mag.pkl\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(model))\n\u001b[1;32m    167\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(mag_modelfile, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m--> 168\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msvd_mag_model \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(handle)\n\u001b[1;32m    170\u001b[0m outdir \u001b[39m=\u001b[39m mag_modelfile\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m \u001b[39mfor\u001b[39;00m filt \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msvd_mag_model\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m~/anaconda3/envs/nmma/lib/python3.8/site-packages/sklearn/base.py:325\u001b[0m, in \u001b[0;36mBaseEstimator.__setstate__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m         \u001b[39mreturn\u001b[39;00m state\n\u001b[0;32m--> 325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__setstate__\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m    326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39msklearn.\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    327\u001b[0m         pickle_version \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_sklearn_version\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpre-0.18\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "df = create_df(jsons, residuals=False)\n",
    "t1 = time.time()\n",
    "print(\"Creating dataframe (no residuals) took {:.2f} seconds\".format(t1-t0))\n",
    "df.to_csv('fit_results.csv', index=False)\n",
    "\n",
    "t0 = time.time()\n",
    "df = create_df(jsons, residuals=True)\n",
    "t1 = time.time()\n",
    "print(\"Creating dataframe (with residuals) took {:.2f} seconds\".format(t1-t0))\n",
    "df.to_csv('fit_results_residuals.csv', index=False)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bu2019lm_0_fit_Bu2019lm_t_11',\n",
       " 'Bu2019lm_0_fit_Bu2019lm_t_13',\n",
       " 'Bu2019lm_0_fit_Bu2019lm_t_15',\n",
       " 'Bu2019lm_0_fit_Bu2019lm_t_3',\n",
       " 'Bu2019lm_0_fit_Bu2019lm_t_5',\n",
       " 'Bu2019lm_0_fit_Bu2019lm_t_7',\n",
       " 'Bu2019lm_0_fit_Bu2019lm_t_9']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[-13.601634765000107,\n",
       " -13.034074737469776,\n",
       " -16.467502801454962,\n",
       " -11.185313315715456,\n",
       " -9.574659288052395,\n",
       " -14.089173384157903,\n",
       " -11.697179303251147]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood_test = ([read_json(j) for j in glob.glob('./fits/Bu2019lm_0_fit_Bu2019lm/*result.json')])\n",
    "\n",
    "sorted_idx = np.argsort([j['label'] for j in log_likelihood_test])\n",
    "log_likelihood_test = [log_likelihood_test[i] for i in sorted_idx]\n",
    "display([j['label'] for j in log_likelihood_test])\n",
    "\n",
    "[np.min(j['log_likelihood_evaluations']) for j in log_likelihood_test]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
